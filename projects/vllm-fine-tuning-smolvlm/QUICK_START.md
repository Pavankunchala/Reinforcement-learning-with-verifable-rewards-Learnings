# Quick Start: SmolVLM-256M ChartQA Fine-Tuning

## ðŸš€ Get Training in 5 Minutes

### 1. Navigate to the Project
```bash
cd Reinforcement-learning-with-verifable-rewards-Learnings/projects/vllm-fine-tuning-smolvlm
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Start Training (Optimized for 16GB GPU)
```bash
python train_smolvlm_chartqa.py
```

**That's it!** The script will:
- âœ… Download SmolVLM-256M model
- âœ… Load ChartQA dataset with lazy loading
- âœ… Fine-tune with optimized parameters
- âœ… Save model to `./smolvlm-256m-chartqa-sft/`

## ðŸ“Š Expected Output

```
ðŸš€ High-Performance Training Configuration:
   â€¢ Batch Size: 16
   â€¢ Learning Rate: 0.001
   â€¢ Epochs: 2
   â€¢ LoRA Rank (r): 16
   â€¢ Memory Target: 14.0GB

ðŸš€ Starting HIGH-PERFORMANCE SmolVLM-256M training...
âœ… Training completed successfully!
ðŸ’¾ Saving final model to smolvlm-256m-chartqa-sft
```

## ðŸ§ª Test Your Model

### Quick Test (10 samples)
```bash
python test_smolvlm_chartqa.py
```

### Extended Test (50 samples)
```bash
python test_smolvlm_chartqa.py --num_samples 50
```

## âš¡ Performance Tips

### For Different GPU Sizes

#### 16GB GPU (Recommended)
```bash
python train_smolvlm_chartqa.py --memory_limit 14.0 --batch_size 16
```

#### 12GB GPU
```bash
python train_smolvlm_chartqa.py --memory_limit 10.0 --batch_size 12
```

#### 8GB GPU
```bash
python train_smolvlm_chartqa.py --memory_limit 6.0 --batch_size 8
```

### For Better Accuracy

#### High-Capacity LoRA
```bash
python train_smolvlm_chartqa.py --lora_r 24 --lora_alpha 48 --epochs 3
```

#### Longer Training
```bash
python train_smolvlm_chartqa.py --max_steps 750 --learning_rate 0.001
```

## ðŸŽ¯ What You Get

After training, you'll have:
- **Fine-tuned SmolVLM-256M** model in `./smolvlm-256m-chartqa-sft/`
- **Test results** in `./smolvlm_256m_test_output/`
- **Performance metrics** and sample predictions
- **Ready-to-use model** for chart understanding tasks

## ðŸ“ˆ Expected Performance

| Metric | Expected | Actual (Recent Test) |
|--------|----------|---------------------|
| Training Time | 15-25 min | ~12 min |
| GPU Memory | <2GB | ~0.5GB |
| Test Accuracy | 40-60% | 40% (4/10) |
| Model Size | 256M params | 256M params |

## ðŸ”§ Troubleshooting

### Common Issues & Solutions

#### Memory Errors
```bash
# Reduce batch size and memory limit
python train_smolvlm_chartqa.py --batch_size 8 --memory_limit 6.0
```

#### Slow Training
```bash
# Use higher learning rate and larger batch size
python train_smolvlm_chartqa.py --learning_rate 0.002 --batch_size 20
```

#### Poor Performance
```bash
# Increase LoRA capacity and training time
python train_smolvlm_chartqa.py --lora_r 24 --lora_alpha 48 --epochs 3 --max_steps 750
```

## ðŸ“š Next Steps

1. **Read the full [README.md](README.md)** for detailed configuration options
2. **Experiment with different LoRA settings** for your specific use case
3. **Check the [main repository](../../README.md)** for other RL approaches
4. **Share your results** and contribute improvements!

## ðŸŽ‰ Success Metrics

**Your training is successful if:**
- âœ… Model loads without errors
- âœ… Training completes in 15-25 minutes
- âœ… GPU memory usage stays under your specified limit
- âœ… Test accuracy is above 30%
- âœ… Model generates reasonable chart-related answers

---

**Happy fine-tuning! ðŸŽ¯**

*Need help? Check the full README.md or open an issue in the main repository.*
